---
title: "CLASSIFICATION OF EXERCISE QUALITY BASED ON ACCELOROMETER DATA"
author: "BG"
date: "September 4th, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
```

## Data Cleaning

First seed was set. Some steps in data preprocessing was carried out externally in MS Excel. First 7 columns (user specific information; output displayed below) were removed prior to splitting the data into Training (80%) and Validation (20%) sets.

```{r cars}
library(caret)
library(AppliedPredictiveModeling)
set.seed(12331)
pml1<-read.csv(sep=",", header=TRUE, "pml-training2.csv")
dim(pml1)
names(pml1[,c(1:10)])
pml1<-pml1[,c(-1:-7)] # Remove columns pertaining to user (name, time window etc.)
dim(pml1)
intrain<-createDataPartition(pml1$classe, list=FALSE,p=0.8)
pmlval<-pml1[-intrain,] ## create validation set
pmltemp<-pml1[intrain,] ## Use remaining data to create Training & Test Data
pmltrain<-pmltemp

dim(pmltrain);dim(pmlval) # Display dimensions of Training and Validation sets
```

## Dimension Reduction using PCA

Cleaned data still consists of 52 predictors and 1 outcome variable (classe). Dimension reduction was performed understand if fewer covariates can be utilized for prediction of the outcome variable. PCA model was created and then utilized to convert the training and validation data into principal components (PCs). Prior to creating PCs, the summary of PCA was evaluated to understand the variance explained (plot provided).

```{r pressure, echo=TRUE}
pcaanalysis<-prcomp(pmltrain[,-53]) # 53rd column is the output variable and hence was excluded from the PCA ; 
plot(pcaanalysis, type="l",main="PCA - Plot of Variance Explained")
pcamodel<-preProcess(pmltrain[,-53], method="pca",pcaComp=52) # Run principal components analysis
trainpcadata<-predict(pcamodel, pmltrain[,-53]) # Create principal components for training data
valpcadata<-predict(pcamodel, pmlval[,-53]) # Create principal components for training data

```

## Set Tuning Parameters for Cross Validation
TrainControl was used to set the CV setting. I used Repeated K fold Cross validation. K = 3 and repeat =3. Higher values were utilized , but with traning time exceeding 1 hour, a lower values were utilized. 
```{r Tuning and CV, echo=TRUE}
fitcontrol<-trainControl(method="repeatedcv", number = 3, repeats =3) # Setting control parameters for the SVM training; 3 Fold, repeated cross validation
```

## Training RBF Kernel SVM Parameters
First SVM is run with RBF kernel to determine the possible values of 'C' & 'Sigma' parameters.The best values were then used to run the model and determine the accuracy

```{r Training SVM RBF, echo=TRUE, eval=TRUE}
svm.tune<-train(x=trainpcadata, y=pmltrain$classe, method = "svmRadial",tuneLength =9, trControl=fitcontrol)#best parameters can be identified by svm.tune$best 
grid<-expand.grid(sigma = c(0.01308048), C= c(32)) # select best tuning parameters for the SVM
svm.tune2<-train(x=trainpcadata, y=pmltrain$classe, method = "svmRadial",tuneGrid=grid, trControl=fitcontrol)
svm.tune2
## CV In-sample Accuracy (including all Principal Components: 98%
```

## Training SVM Linear Kernel Parameters
Linear Kernel is much simpler with C = 1. The training time with CV was much shorter when compared to the RBF Kernel , but accuracy was much lower. Since, training time was lower, the outout

```{r Tuning Linear, echo=TRUE}
svmlinear.tune<-train(x=trainpcadata, y=pmltrain$classe, method = "svmLinear",tuneLength =9, trControl=fitcontrol) # Use the Linear Kernel for the SVM vector
svmlinear.tune
```