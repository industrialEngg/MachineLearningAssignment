---
title: "Data Cleaning - ML Assignment"
author: "BG"
date: "September 7, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction

```{r Introduction }
library(caret)
library(AppliedPredictiveModeling)
set.seed(12331)
```

## Data Cleaning
First seed was set. Columns consisting of NA or spaces were eliminated.
First 7 columns (user specific information; output displayed below) were removed prior to splitting the data into Training (70%) and Validation (30%) sets.

```{r Clean, eval=FALSE}

setwd("C:/Users/bag9031/Documents/Personal/Coursera - ML")
pml1<-read.csv(sep=",", header=TRUE, "pml-training.csv",na.string=c("NA", ""))
dim(pml1)
for(i in 1:160) {h1[i]<-length(pml1[,i][pml1[,i]=='NA'])}
df<-data.frame(names(pml1), h1)
colnames(df)<-c("name","count")
df2<-subset(df, count==0)
pml2<-pml1[,as.character(df2$name)]
pml2<-pml2[,c(-1:-7)] # Remove columns pertaining to user (name, time window etc.)
dim(pml2)
write.table(pml2, "C:/Users/bag9031/Documents/Personal/Coursera - ML/Testpml" ) ## save file as table for future use
```

##Read data and split into Training and Validation sets
```{r reading, echo=TRUE}
setwd("C:/Users/bag9031/Documents/Personal/Coursera - ML")
p<-read.table("Testpml")
pml2<-as.data.frame(p)
dim(pml2)
intrain<-createDataPartition(pml2$classe, list=FALSE,p=0.3)
pmlval<-pml2[-intrain,] ## create validation set
pmltemp<-pml2[intrain,] ## Use remaining data to create Training & Test Data
pmltrain<-pmltemp

dim(pmltrain)
dim(pmlval) # Display dimensions of Training and Validation sets
```

## Dimension Reduction using PCA

Cleaned data still consists of 52 predictors and 1 outcome variable (classe). Dimension reduction was performed understand if fewer covariates can be utilized for prediction of the outcome variable. PCA model was created and then utilized to convert the training and validation data into principal components (PCs). Prior to creating PCs, the summary of PCA was evaluated to understand the variance explained (plot provided).

```{r pressure, echo=TRUE}
pcaanalysis<-prcomp(pmltrain[,-53]) # 53rd column is the output variable and hence was excluded from the PCA ; 
plot(pcaanalysis, type="l",main="PCA - Plot of Variance Explained")
pcamodel<-preProcess(pmltrain[,-53], method="pca",pcaComp=52) # Run principal components analysis
trainpcadata<-predict(pcamodel, pmltrain[,-53]) # Create principal components for training data
valpcadata<-predict(pcamodel, pmlval[,-53]) # Create principal components for training data

```

## Set Tuning Parameters for Cross Validation
TrainControl was used to set the CV setting. I used Repeated K fold Cross validation. K = 3 and repeat =3. Higher values were utilized , but with traning time exceeding 1 hour, a lower values were utilized. 

```{r Tuning and CV, echo=TRUE, eval=TRUE}
fitcontrolrbf<-trainControl(method="cv", number = 10) # Setting control parameters for the SVM training; 10 Fold, cross validation
fitcontrollinear<-trainControl(method="repeatedcv", number = 10, repeats =3) # Setting control parameters for the SVM training; 3 Fold, repeated cross validation
```

## Training RBF Kernel SVM Parameters
First SVM is run with RBF kernel to determine the possible values of 'C' & 'Sigma' parameters.The best values were then used to run the model and determine the accuracy

```{r Training SVM RBF, echo=TRUE, eval=TRUE}
svm.tune<-train(x=trainpcadata, y=pmltrain$classe, method = "svmRadial",tuneLength =9, trControl=fitcontrolrbf)#best parameters can be identified by svm.tune$best 
grid<-expand.grid(sigma = c(0.01308048), C= c(32)) # select best tuning parameters for the SVM
svm.tune2<-train(x=trainpcadata, y=pmltrain$classe, method = "svmRadial",tuneGrid=grid, trControl=fitcontrolrbf)
svm.tune2
## CV In-sample Accuracy (including all Principal Components: 98%
```

## Training SVM Linear Kernel Parameters
Linear Kernel is much simpler with C = 1. The training time with CV was much shorter when compared to the RBF Kernel , but accuracy was much lower. Since, training time was lower, the outout

```{r Tuning Linear, echo=TRUE}
svmlinear.tune<-train(x=trainpcadata, y=pmltrain$classe, method = "svmLinear",tuneLength =9, trControl=fitcontrollinear) # Use the Linear Kernel for the SVM vector
svmlinear.tune
```


## Out of Sample Error Estimation for SVM w RBF Kernel
Out of Sample Accuracy & Error were estimated using the Validation (Test) data set previously created.
First Out of Sample Error was estimated for the SVM model with RBF Kernel 
```{r Out of Sample RBF, echo=TRUE}
cfmmatrixsvmrbf<-confusionMatrix(pmlval$classe, predict(svm.tune2, newdata=valpcadata))
cfmmatrixsvmrbf

```
Error for SVM w RBF Kernel is

```{r Out of Error RBF, echo=TRUE}
err1svmrbf<-1-as.numeric(cfmmatrixsvmrbf$overall[1])
err1svmrbf
```


## Out of Sample Error Estimation for SVM w Linear Kernel
Out of Sample Accuracy & Error for the Linear Kernel RBFwere estimated using the Validation (Test) data set previously created.
```{r Out of CFM Linear, echo=TRUE}

cfmmatrixsvmlinear<-confusionMatrix(pmlval$classe, predict(svmlinear.tune, newdata=valpcadata))
cfmmatrixsvmlinear

```
Error for SVM w Linear Kernel is

```{r Out of Sample Linear, echo=TRUE}
err1svmlinear<-1-as.numeric(cfmmatrixsvmlinear$overall[1])
err1svmlinear
```
